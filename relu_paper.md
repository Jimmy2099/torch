# Famous Papers on Rectified Linear Unit (ReLU)

## 1. Deep Sparse Rectifier Neural Networks
- **Authors**: Xavier Glorot, Antoine Bordes, Yoshua Bengio
- **Conference**: AISTATS 2011
- **Link**: [arXiv:1502.01852](https://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf)
- **Summary**: This paper demonstrated the advantages of ReLU over sigmoid and tanh, showing that ReLU mitigates the vanishing gradient problem and promotes sparsity in activations, leading to better generalization in deep networks.

## 2. Rectified Linear Units Improve Restricted Boltzmann Machines
- **Authors**: Vinod Nair, Geoffrey E. Hinton
- **Conference**: ICML 2010
- **Link**: [Paper](https://www.cs.toronto.edu/~hinton/absps/reluICML.pdf)
- **Summary**: This paper introduced the use of ReLU in Restricted Boltzmann Machines (RBMs) and demonstrated its advantages in training efficiency and performance compared to traditional activation functions.

## 3. ImageNet Classification with Deep Convolutional Neural Networks
- **Authors**: Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton
- **Conference**: NIPS 2012 (NeurIPS)
- **Link**: [Paper](https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)
- **Summary**: This paper introduced AlexNet, which heavily relied on ReLU to train deep convolutional neural networks, demonstrating breakthrough performance in large-scale image classification.

## 4. Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification
- **Authors**: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
- **Conference**: ICCV 2015
- **Link**: [arXiv:1502.01852](https://arxiv.org/abs/1502.01852)
- **Summary**: This paper introduced Parametric ReLU (PReLU) and analyzed the effects of different types of rectifier units on deep learning performance, leading to state-of-the-art results on ImageNet.

## 5. Understanding the Exploding Gradient Problem
- **Authors**: Xavier Glorot, Yoshua Bengio
- **Conference**: AISTATS 2010
- **Link**: [Paper](https://arxiv.org/pdf/1211.5063v1)
- **Summary**: This paper discussed the exploding and vanishing gradient problem in deep networks and provided insights into weight initialization techniques that work well with ReLU.
